{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "from __future__ import print_function \n",
    "import argparse \n",
    "import torch \n",
    "from torch import cuda\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "from torchvision import datasets, transforms \n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings \n",
    "batch_size = 64\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# MNIST Dataset \n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                               train=True,\n",
    "                              transform=transforms.ToTensor(),\n",
    "                              download=False)\n",
    "test_dataset = datasets.MNIST(root='./data/',\n",
    "                             train=False,\n",
    "                             transform=transforms.ToTensor()) \n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False)\n",
    "# Model generation\n",
    "class InceptionA(nn.Module):\n",
    "    \n",
    "    def __init__(self,in_channels):\n",
    "        super(InceptionA,self).__init__()\n",
    "        self.branch1x1 = nn.Conv2d(in_channels,16,kernel_size=1)\n",
    "        \n",
    "        self.branch5x5_1 = nn.Conv2d(in_channels,16,kernel_size=1)\n",
    "        self.branch5x5_2 = nn.Conv2d(16,24,kernel_size=5,padding=2)\n",
    "        \n",
    "        self.branch3x3db1_1 = nn.Conv2d(in_channels,16,kernel_size=1)\n",
    "        self.branch3x3db1_2 = nn.Conv2d(16,24,kernel_size=3,padding=1)\n",
    "        self.branch3x3db1_3 = nn.Conv2d(24,24,kernel_size=3,padding=1)\n",
    "        \n",
    "        self.branch_pool = nn.Conv2d(in_channels, 24, kernel_size=1)\n",
    "        \n",
    "        #kernel_size =1 means a 1x1 convolution filter\n",
    "        # it can perform a  \"Number of depth-demensional dot product\"\n",
    "        \n",
    "    def forward(self,x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "        #print(\"branch1x1 shape {}\".format(branch1x1.shape))\n",
    "        \n",
    "        branch5x5 = self.branch5x5_1(x)\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\n",
    "        #print(\"{} shape {}\".format(branch5x5,branch5x5.shape))\n",
    "        \n",
    "        branch3x3db1 = self.branch3x3db1_1(x)\n",
    "        branch3x3db1 = self.branch3x3db1_2(branch3x3db1)\n",
    "        branch3x3db1 = self.branch3x3db1_3(branch3x3db1)\n",
    "        #print(\"{} shape {}\".format(branch3x3db1,branch3x3db1.shape))\n",
    "        \n",
    "        branch_pool = F.avg_pool2d(x,kernel_size=3,stride=1,padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "        #print(\"{} shape {}\".format(branch_pool,branch_pool.shape))\n",
    "        \n",
    "        outputs = [branch1x1, branch5x5, branch3x3db1, branch_pool]\n",
    "        #print(\"{} shape {}\".format(outputs,np.shape(outputs)))\n",
    "        \n",
    "        return torch.cat(outputs,1)  \n",
    "        #torch.cat(tensors, dim=0, out=None) -> Tensor \n",
    "        #dim (int, optional): the dimension over which the tensors are concatenated\n",
    "        # dim = 0 이면 인풋 그대로 가져다 붙이고 [[x 1행], [x 2행], [y 1행],[y 2행],[z 1행], [z 2행]] 이런식.  \n",
    "        # dim = 1 이면 [[x 1행, y 1행, z 1행],[x 2행, y 2행, z 2행]] 이런식으로 concatenation 됨         \n",
    "        \n",
    "        \n",
    "class Net(nn.Module): \n",
    "    \n",
    "    def __init__(self): \n",
    "        super(Net,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,10,kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(88,20,kernel_size=5) #inception output size is 88, 16+24+24+24\n",
    "        \n",
    "        self.incept1 = InceptionA(in_channels=10)\n",
    "        self.incept2 = InceptionA(in_channels=20)\n",
    "        \n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(1408,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.mp(self.conv1(x)))   #conv1-maxpooling-relu\n",
    "        x = self.incept1(x)\n",
    "        x = F.relu(self.mp(self.conv2(x)))  #conv2-maxpooling-relu\n",
    "        x = self.incept2(x)\n",
    "        x = x.view(in_size,-1)\n",
    "        x = self.fc(x) \n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "# Model initialization     \n",
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.01,momentum=0.5)\n",
    "\n",
    "# train \n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data,target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0: \n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "# test            \n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0 \n",
    "    correct = 0 \n",
    "    for data, target in test_loader: \n",
    "        data, target = Variable(data, volatile = True), Variable(target) \n",
    "                                    # volatile ?? \n",
    "        output = model(data) \n",
    "        # sum up batch loss \n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0]\n",
    "                                                # size_average ??? \n",
    "        #get the index of the max log-probability \n",
    "        pred = output.data.max(1,keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "# main    \n",
    "if __name__ is '__main__': \n",
    "    for epoch in range(1,10):\n",
    "        train(epoch)\n",
    "        test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2736, -0.0132,  0.1037,  0.0000,  0.0000,  0.0000,  1.0000,  1.0000,\n",
       "          1.0000],\n",
       "        [ 2.4795,  0.2064,  0.4394,  0.0000,  0.0000,  0.0000,  1.0000,  1.0000,\n",
       "          1.0000],\n",
       "        [ 0.4044, -0.3779,  0.6910,  0.0000,  0.0000,  0.0000,  1.0000,  1.0000,\n",
       "          1.0000],\n",
       "        [-0.8426, -0.5520,  0.1906,  0.0000,  0.0000,  0.0000,  1.0000,  1.0000,\n",
       "          1.0000],\n",
       "        [ 0.1362, -0.6483,  0.1413,  0.0000,  0.0000,  0.0000,  1.0000,  1.0000,\n",
       "          1.0000],\n",
       "        [ 0.3161, -0.6680,  2.5854,  0.0000,  0.0000,  0.0000,  1.0000,  1.0000,\n",
       "          1.0000]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "```python\n",
    "x = torch.randn(6,3)\n",
    "y = torch.zeros(6,3)\n",
    "z = torch.ones(6,3)\n",
    "torch.cat((x,y,z),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![inception_architecture](./img/inception-module_.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![inception module](./img/inception-module.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image 출처 :https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![inception-module](./img/stacking_layer_problem.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![inception-module](./img/sol1_depp_residual_layer.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![inception-module](./img/Imagenet_experiments.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image 출처 https://www.youtube.com/watch?v=hqYfqNAQIjE&list=PLlMkM4tgfjnJ3I-dbhO9JTw7gNty6o_2m&index=11"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "develroom-pytorch",
   "language": "python",
   "name": "develroom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
