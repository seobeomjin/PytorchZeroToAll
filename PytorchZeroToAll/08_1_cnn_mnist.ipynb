{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "from __future__ import print_function \n",
    "import argparse \n",
    "'''\n",
    "python train.py --epochs 50 --batch-size 64 --save-dir weights \n",
    "We can see like this code with machine learning python script \n",
    "Many hyperparameters including training process would be setted with \n",
    "\"python scrip_name.py --with some options\" Thus we need to understand about argparse\n",
    "'''\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "from torchvision import datasets, transforms \n",
    "from torch.autograd import Variable \n",
    "\n",
    "\n",
    "#Training settings \n",
    "batch_size = 64 \n",
    "\n",
    "#MNIST Dataset \n",
    "train_dataset = datasets.MNIST(root = './data/',\n",
    "                              train = True, \n",
    "                              transform = transforms.ToTensor(),\n",
    "                              download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root= './data/',\n",
    "                             train = False, \n",
    "                             transform = transforms.ToTensor()) \n",
    "\n",
    "#Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "                                          batch_size = batch_size, \n",
    "                                          shuffle = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "                                         batch_size = batch_size, \n",
    "                                         shuffle = False)\n",
    "\n",
    "#Model generation  \n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self): \n",
    "        super(Net,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,10,kernel_size = 5)\n",
    "        self.conv2 = nn.Conv2d(10,20,kernel_size = 5)\n",
    "        self.mp = nn.MaxPool2d(2) \n",
    "        self.fc = nn.Linear(320,10)\n",
    "        \n",
    "        #self.check_x_size = [] \n",
    "        #self.check_x_view = []\n",
    "        \n",
    "        '''\n",
    "        torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, \n",
    "        padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "        \n",
    "        torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, \n",
    "        return_indices=False, ceil_mode=False)\n",
    "        '''\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        # addition for checking\n",
    "        #self.check_x_size.append(x.size(0))\n",
    "        \n",
    "        in_size = x.size(0) # number of rows  \n",
    "        x = F.relu(self.mp(self.conv1(x)))\n",
    "        x = F.relu(self.mp(self.conv2(x)))\n",
    "        x = x.view(in_size,-1) #flatten the tensor \n",
    "        \n",
    "        '''\n",
    "        x.size(0) \n",
    "         >> x.size(0) 은 2차원 행렬에서 row의 갯수를 반환. \n",
    "         >> x.size(1) 은 2차원 행렬에서 col의 갯수를 반환. \n",
    "         \n",
    "         \n",
    "        x.view(x.size(0),-1)  -> view is same with \"reshape\"\n",
    "        \n",
    "        tensor.view(a,b) \n",
    "         >> If you want to reshape this tensor to make it (a,b)\n",
    "        \n",
    "        x.view(x.size(0),-1)   \n",
    "         >> number of row 는 x.size(0) 수 만큼 맞추고, cols는 앞의 조건에 맞춰 되는대로 많이 ,, \n",
    "        \n",
    "        \n",
    "        If there is any situation that you don't know how many rows you want but \n",
    "        are sure of the number of columns, then you can specify this with a -1. \n",
    "        (Note that you can extend this to tensors with more dimensions. Only one of the axis value can be -1).\n",
    "        \n",
    "        You have to flatten this to give it to the fully connected layer. \n",
    "        So you tell pytorch to reshape the tensor you obtained to have \n",
    "        specific number of columns and tell it to decide the number of rows by itself.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        x = self.fc(x) \n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "    \n",
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5) \n",
    "\n",
    "def train(epoch): \n",
    "    model.train()\n",
    "    for batch_idx, (data,target) in enumerate(train_loader): \n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0 : \n",
    "            print('Train Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx*len(data), len(train_loader.dataset),\n",
    "            100.*batch_idx / len(train_loader),loss.item()))\n",
    "\n",
    "            \n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0 \n",
    "    correct = 0 \n",
    "    for data, target in test_loader : \n",
    "        data, target = Variable(data), Variable(target)\n",
    "        output = model(data)\n",
    "        #sum up batch loss \n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data\n",
    "        #get the index og the max log_probability \n",
    "        pred = output.data.max(1,keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set : Average loss : {:.4f}, Accuracy : {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100.* correct/len(test_loader.dataset)))\n",
    "    \n",
    "if __name__ == '__main__': \n",
    "    for epoch in range(1,10): \n",
    "        train(epoch)\n",
    "        test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.7797, -0.3800, -0.2669,  0.5075, -1.7320],\n",
      "         [-0.7702,  0.3129, -1.7766,  0.2101,  1.1058],\n",
      "         [-0.9809,  0.1280, -0.0468,  0.3478,  1.3486]],\n",
      "\n",
      "        [[-0.0380, -0.7387, -1.2794,  0.3995, -0.9839],\n",
      "         [-0.0183, -0.9246, -0.4916, -0.1553, -1.1914],\n",
      "         [-0.3024,  0.0967, -0.7624,  0.2536,  2.8452]]])\n",
      "t.size(0) : 2\tt.size(1): 3\tt.size(2): 5\n"
     ]
    }
   ],
   "source": [
    "t = torch.randn([2,3,5])\n",
    "print(t)\n",
    "print(\"t.size(0) : {}\\tt.size(1): {}\\tt.size(2): {}\".format(t.size(0),t.size(1),t.size(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주석참조주소<br> https://greeksharifa.github.io/pytorch/2018/11/10/pytorch-usage-03-How-to-Use-PyTorch/\n",
    "\n",
    "argparse 및 pytorch model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![aboout torch conv](kernel_size.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "develroom-pytorch",
   "language": "python",
   "name": "develroom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
