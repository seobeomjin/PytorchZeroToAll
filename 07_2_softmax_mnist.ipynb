{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MNIST on cpu\n",
      " ========================================\n",
      "Train Epoch : 1 | Batch Status : 0/60000 (0%) | Loss : 2.294512\n",
      "Train Epoch : 1 | Batch Status : 640/60000 (1%) | Loss : 2.293311\n",
      "Train Epoch : 1 | Batch Status : 1280/60000 (2%) | Loss : 2.316295\n",
      "Train Epoch : 1 | Batch Status : 1920/60000 (3%) | Loss : 2.301678\n",
      "Train Epoch : 1 | Batch Status : 2560/60000 (4%) | Loss : 2.306928\n",
      "Train Epoch : 1 | Batch Status : 3200/60000 (5%) | Loss : 2.305601\n",
      "Train Epoch : 1 | Batch Status : 3840/60000 (6%) | Loss : 2.312315\n",
      "Train Epoch : 1 | Batch Status : 4480/60000 (7%) | Loss : 2.300719\n",
      "Train Epoch : 1 | Batch Status : 5120/60000 (9%) | Loss : 2.298192\n",
      "Train Epoch : 1 | Batch Status : 5760/60000 (10%) | Loss : 2.301229\n",
      "Train Epoch : 1 | Batch Status : 6400/60000 (11%) | Loss : 2.303575\n",
      "Train Epoch : 1 | Batch Status : 7040/60000 (12%) | Loss : 2.301944\n",
      "Train Epoch : 1 | Batch Status : 7680/60000 (13%) | Loss : 2.303360\n",
      "Train Epoch : 1 | Batch Status : 8320/60000 (14%) | Loss : 2.298308\n",
      "Train Epoch : 1 | Batch Status : 8960/60000 (15%) | Loss : 2.301143\n",
      "Train Epoch : 1 | Batch Status : 9600/60000 (16%) | Loss : 2.293283\n",
      "Train Epoch : 1 | Batch Status : 10240/60000 (17%) | Loss : 2.300119\n",
      "Train Epoch : 1 | Batch Status : 10880/60000 (18%) | Loss : 2.295650\n",
      "Train Epoch : 1 | Batch Status : 11520/60000 (19%) | Loss : 2.296839\n",
      "Train Epoch : 1 | Batch Status : 12160/60000 (20%) | Loss : 2.296061\n",
      "Train Epoch : 1 | Batch Status : 12800/60000 (21%) | Loss : 2.293431\n",
      "Train Epoch : 1 | Batch Status : 13440/60000 (22%) | Loss : 2.294609\n",
      "Train Epoch : 1 | Batch Status : 14080/60000 (23%) | Loss : 2.292397\n",
      "Train Epoch : 1 | Batch Status : 14720/60000 (25%) | Loss : 2.298342\n",
      "Train Epoch : 1 | Batch Status : 15360/60000 (26%) | Loss : 2.297401\n",
      "Train Epoch : 1 | Batch Status : 16000/60000 (27%) | Loss : 2.294757\n",
      "Train Epoch : 1 | Batch Status : 16640/60000 (28%) | Loss : 2.291699\n",
      "Train Epoch : 1 | Batch Status : 17280/60000 (29%) | Loss : 2.301351\n",
      "Train Epoch : 1 | Batch Status : 17920/60000 (30%) | Loss : 2.292212\n",
      "Train Epoch : 1 | Batch Status : 18560/60000 (31%) | Loss : 2.289778\n",
      "Train Epoch : 1 | Batch Status : 19200/60000 (32%) | Loss : 2.293146\n",
      "Train Epoch : 1 | Batch Status : 19840/60000 (33%) | Loss : 2.288985\n",
      "Train Epoch : 1 | Batch Status : 20480/60000 (34%) | Loss : 2.289005\n",
      "Train Epoch : 1 | Batch Status : 21120/60000 (35%) | Loss : 2.291980\n",
      "Train Epoch : 1 | Batch Status : 21760/60000 (36%) | Loss : 2.283075\n",
      "Train Epoch : 1 | Batch Status : 22400/60000 (37%) | Loss : 2.288046\n",
      "Train Epoch : 1 | Batch Status : 23040/60000 (38%) | Loss : 2.285301\n",
      "Train Epoch : 1 | Batch Status : 23680/60000 (39%) | Loss : 2.286350\n",
      "Train Epoch : 1 | Batch Status : 24320/60000 (41%) | Loss : 2.288939\n",
      "Train Epoch : 1 | Batch Status : 24960/60000 (42%) | Loss : 2.282597\n",
      "Train Epoch : 1 | Batch Status : 25600/60000 (43%) | Loss : 2.280963\n",
      "Train Epoch : 1 | Batch Status : 26240/60000 (44%) | Loss : 2.278457\n",
      "Train Epoch : 1 | Batch Status : 26880/60000 (45%) | Loss : 2.287839\n",
      "Train Epoch : 1 | Batch Status : 27520/60000 (46%) | Loss : 2.284033\n",
      "Train Epoch : 1 | Batch Status : 28160/60000 (47%) | Loss : 2.282718\n",
      "Train Epoch : 1 | Batch Status : 28800/60000 (48%) | Loss : 2.275077\n",
      "Train Epoch : 1 | Batch Status : 29440/60000 (49%) | Loss : 2.277081\n",
      "Train Epoch : 1 | Batch Status : 30080/60000 (50%) | Loss : 2.266306\n",
      "Train Epoch : 1 | Batch Status : 30720/60000 (51%) | Loss : 2.273140\n",
      "Train Epoch : 1 | Batch Status : 31360/60000 (52%) | Loss : 2.271592\n",
      "Train Epoch : 1 | Batch Status : 32000/60000 (53%) | Loss : 2.262174\n",
      "Train Epoch : 1 | Batch Status : 32640/60000 (54%) | Loss : 2.272607\n",
      "Train Epoch : 1 | Batch Status : 33280/60000 (55%) | Loss : 2.266093\n",
      "Train Epoch : 1 | Batch Status : 33920/60000 (57%) | Loss : 2.266835\n",
      "Train Epoch : 1 | Batch Status : 34560/60000 (58%) | Loss : 2.257178\n",
      "Train Epoch : 1 | Batch Status : 35200/60000 (59%) | Loss : 2.280481\n",
      "Train Epoch : 1 | Batch Status : 35840/60000 (60%) | Loss : 2.260971\n",
      "Train Epoch : 1 | Batch Status : 36480/60000 (61%) | Loss : 2.255713\n",
      "Train Epoch : 1 | Batch Status : 37120/60000 (62%) | Loss : 2.272174\n",
      "Train Epoch : 1 | Batch Status : 37760/60000 (63%) | Loss : 2.256404\n",
      "Train Epoch : 1 | Batch Status : 38400/60000 (64%) | Loss : 2.250079\n",
      "Train Epoch : 1 | Batch Status : 39040/60000 (65%) | Loss : 2.249965\n",
      "Train Epoch : 1 | Batch Status : 39680/60000 (66%) | Loss : 2.225165\n",
      "Train Epoch : 1 | Batch Status : 40320/60000 (67%) | Loss : 2.252969\n",
      "Train Epoch : 1 | Batch Status : 40960/60000 (68%) | Loss : 2.228109\n",
      "Train Epoch : 1 | Batch Status : 41600/60000 (69%) | Loss : 2.236927\n",
      "Train Epoch : 1 | Batch Status : 42240/60000 (70%) | Loss : 2.228084\n",
      "Train Epoch : 1 | Batch Status : 42880/60000 (71%) | Loss : 2.222927\n",
      "Train Epoch : 1 | Batch Status : 43520/60000 (72%) | Loss : 2.202387\n",
      "Train Epoch : 1 | Batch Status : 44160/60000 (74%) | Loss : 2.188228\n",
      "Train Epoch : 1 | Batch Status : 44800/60000 (75%) | Loss : 2.195435\n",
      "Train Epoch : 1 | Batch Status : 45440/60000 (76%) | Loss : 2.218365\n",
      "Train Epoch : 1 | Batch Status : 46080/60000 (77%) | Loss : 2.169117\n",
      "Train Epoch : 1 | Batch Status : 46720/60000 (78%) | Loss : 2.145156\n",
      "Train Epoch : 1 | Batch Status : 47360/60000 (79%) | Loss : 2.134807\n",
      "Train Epoch : 1 | Batch Status : 48000/60000 (80%) | Loss : 2.119428\n",
      "Train Epoch : 1 | Batch Status : 48640/60000 (81%) | Loss : 2.142936\n",
      "Train Epoch : 1 | Batch Status : 49280/60000 (82%) | Loss : 2.099062\n",
      "Train Epoch : 1 | Batch Status : 49920/60000 (83%) | Loss : 2.075182\n",
      "Train Epoch : 1 | Batch Status : 50560/60000 (84%) | Loss : 2.065456\n",
      "Train Epoch : 1 | Batch Status : 51200/60000 (85%) | Loss : 2.002343\n",
      "Train Epoch : 1 | Batch Status : 51840/60000 (86%) | Loss : 1.944462\n",
      "Train Epoch : 1 | Batch Status : 52480/60000 (87%) | Loss : 1.926860\n",
      "Train Epoch : 1 | Batch Status : 53120/60000 (88%) | Loss : 1.888890\n",
      "Train Epoch : 1 | Batch Status : 53760/60000 (90%) | Loss : 1.849698\n",
      "Train Epoch : 1 | Batch Status : 54400/60000 (91%) | Loss : 1.800609\n",
      "Train Epoch : 1 | Batch Status : 55040/60000 (92%) | Loss : 1.825973\n",
      "Train Epoch : 1 | Batch Status : 55680/60000 (93%) | Loss : 1.658916\n",
      "Train Epoch : 1 | Batch Status : 56320/60000 (94%) | Loss : 1.627554\n",
      "Train Epoch : 1 | Batch Status : 56960/60000 (95%) | Loss : 1.666799\n",
      "Train Epoch : 1 | Batch Status : 57600/60000 (96%) | Loss : 1.629589\n",
      "Train Epoch : 1 | Batch Status : 58240/60000 (97%) | Loss : 1.463005\n",
      "Train Epoch : 1 | Batch Status : 58880/60000 (98%) | Loss : 1.565495\n",
      "Train Epoch : 1 | Batch Status : 59520/60000 (99%) | Loss : 1.340387\n",
      "Training time : 0m 10s\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'tset_loss' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b4f3885402e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    126\u001b[0m       \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdivmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepoch_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Training time : {m:.0f}m {s:.0f}s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m       \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdivmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0meopch_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Testing time: {m:.0f}m {s:.0f}s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-b4f3885402e2>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m#sum up batch loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mtset_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;31m#get the index of the max\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'tset_loss' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "from __future__ import print_function \n",
    "'''\n",
    "using for python 3.x keywords and features in python 2. > for compatibility \n",
    "'''\n",
    "from torch import nn, optim, cuda \n",
    "from torch.utils import data \n",
    "'''\n",
    "in this module, there are Dataset and DataLoader ,, etc.\n",
    "data.Dataset >> is using for virtual class for custom data\n",
    "data.DataLoader >> is using for input Pipeline\n",
    "'''\n",
    "from torchvision import datasets, transforms \n",
    "'''\n",
    "The torchvision package consists of popular datasets, model architectures, \n",
    "and common image transformations for computer vision.\n",
    "'''\n",
    "import torch.nn.functional as F \n",
    "import time\n",
    "\n",
    "#Training settings \n",
    "batch_size = 64 \n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(f'Training MNIST on {device}\\n {\"=\"*40}')\n",
    "\n",
    "#MNIST Dataset \n",
    "train_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                              train=True,\n",
    "                              transform=transforms.ToTensor(),\n",
    "                              download=True)\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                             train=False,\n",
    "                             transform=transforms.ToTensor())\n",
    "\n",
    "#Data Loader (Input Pipeline)\n",
    "train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                            batch_size = batch_size,\n",
    "                            shuffle=True)\n",
    "\n",
    "test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=False)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.l1 = nn.Linear(784,520)\n",
    "        self.l2 = nn.Linear(520,320)\n",
    "        self.l3 = nn.Linear(320,240)\n",
    "        self.l4 = nn.Linear(240,120)\n",
    "        self.l5 = nn.Linear(120,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.view(-1,784) # Flatten the data (n,1,28,28) -> (n,784) \n",
    "        ## i think > n is batch_size,, (?)\n",
    "        ## such as view of DataBase (?) \n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        return self.l5(x)\n",
    "    \n",
    "model = Net()\n",
    "model.to(device)  \n",
    "## i think > it send model to device\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.01,momentum=0.5)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()  # >> i think ,, overriding\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        #sending data and target to device\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0: \n",
    "            print('Train Epoch : {} | Batch Status : {}/{} ({:.0f}%) | Loss : {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.item())) \n",
    "      \n",
    "      '''\n",
    "        Reference) \n",
    "        \n",
    "        for i,data in enumerate(train_loader,0):\n",
    "            #get the inputs \n",
    "            inputs, labels = data\n",
    "        \n",
    "        the data shape looks like,, \n",
    "        [tensor([[],[],...,[]]),tensor([[],[],[],...,[]])]\n",
    "        for inputs and then for labels \n",
    "        \n",
    "        >>>  list(tensor(inputs), tensor(data)) <<< \n",
    "        type(data) >> class 'list'\n",
    "        type(data[0]) >> Torch.Tensor\n",
    "        '''\n",
    "            \n",
    "def test():\n",
    "      global test_loss \n",
    "\n",
    "      model.eval()\n",
    "      test_loss = 0 \n",
    "      correct = 0\n",
    "      for data, target in test_loader : \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        #sum up batch loss \n",
    "        tset_loss += criterion(output,target).item()\n",
    "        #get the index of the max\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "        '''\n",
    "        ###### ============== ########\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "        ''' \n",
    "      \n",
    "      test_loss /= len(test_loader.dataset)\n",
    "      print(f'======================\\nTest set: Average loss : {test_loss:.4f},Accuracy:{correct}/{len(test_loader.dataset)}'\n",
    "         f'({100.*correct / len(test_loader.dataset):.0f}%)')\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    since = time.time()\n",
    "    for epoch in range(1,10):\n",
    "      epoch_start = time.time()\n",
    "      train(epoch)\n",
    "      m,s = divmod(time.time() - epoch_start, 60)\n",
    "      print(f'Training time : {m:.0f}m {s:.0f}s')\n",
    "      test()\n",
    "      m, s = divmod(time.time() - eopch_start, 60)\n",
    "      print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
    "      \n",
    "      '''\n",
    "       takes two parameters x and y, where x is treated as \n",
    "       numerator and y is treated as denominator. \n",
    "       The method calculates both x / y and x % y and \n",
    "       returns both the values\n",
    "       \n",
    "       divmod(x, y)\n",
    "       >>>\n",
    "       (x / y, x % y)\n",
    "       \n",
    "      '''\n",
    "        \n",
    "    m,s = divmod(time.time() - since, 60)\n",
    "    print(f'Total Time: {m:.0f}m {s:.0f}s \\n Model was trained on {device}!')\n",
    "        \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'tset_loss' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fbd55f77ab7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-083029f9e262>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m#sum up batch loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mtset_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;31m#get the index of the max\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'tset_loss' referenced before assignment"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1663,  1.1857,  1.0400,  0.5672],\n",
      "        [-1.5602, -0.8451,  0.3930, -0.1837],\n",
      "        [ 0.2274, -0.4366, -1.2270, -0.2365],\n",
      "        [-0.1978,  0.4887,  1.2223,  0.7826]]) \n",
      " tensor([ 0.1663,  1.1857,  1.0400,  0.5672, -1.5602, -0.8451,  0.3930, -0.1837,\n",
      "         0.2274, -0.4366, -1.2270, -0.2365, -0.1978,  0.4887,  1.2223,  0.7826]) \n",
      " tensor([[ 0.1663,  1.1857,  1.0400,  0.5672, -1.5602, -0.8451,  0.3930, -0.1837],\n",
      "        [ 0.2274, -0.4366, -1.2270, -0.2365, -0.1978,  0.4887,  1.2223,  0.7826]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(4,4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1,8)\n",
    "\n",
    "print(x,'\\n',y,'\\n',z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
