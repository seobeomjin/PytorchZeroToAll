{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss : 48.96630859375\n",
      "Epoch: 1 | Loss : 21.809722900390625\n",
      "Epoch: 2 | Loss : 9.72021770477295\n",
      "Epoch: 3 | Loss : 4.33815336227417\n",
      "Epoch: 4 | Loss : 1.9420541524887085\n",
      "Epoch: 5 | Loss : 0.8752221465110779\n",
      "Epoch: 6 | Loss : 0.40014591813087463\n",
      "Epoch: 7 | Loss : 0.18850472569465637\n",
      "Epoch: 8 | Loss : 0.09413868933916092\n",
      "Epoch: 9 | Loss : 0.05198286846280098\n",
      "Epoch: 10 | Loss : 0.03307148069143295\n",
      "Epoch: 11 | Loss : 0.024509858340024948\n",
      "Epoch: 12 | Loss : 0.020557938143610954\n",
      "Epoch: 13 | Loss : 0.018659915775060654\n",
      "Epoch: 14 | Loss : 0.017678314819931984\n",
      "Epoch: 15 | Loss : 0.017106657847762108\n",
      "Epoch: 16 | Loss : 0.016719460487365723\n",
      "Epoch: 17 | Loss : 0.016416218131780624\n",
      "Epoch: 18 | Loss : 0.016152294352650642\n",
      "Epoch: 19 | Loss : 0.01590774953365326\n",
      "Epoch: 20 | Loss : 0.01567351445555687\n",
      "Epoch: 21 | Loss : 0.015445776283740997\n",
      "Epoch: 22 | Loss : 0.015222672373056412\n",
      "Epoch: 23 | Loss : 0.015003475360572338\n",
      "Epoch: 24 | Loss : 0.014787609688937664\n",
      "Epoch: 25 | Loss : 0.014574993401765823\n",
      "Epoch: 26 | Loss : 0.014365517534315586\n",
      "Epoch: 27 | Loss : 0.014159058220684528\n",
      "Epoch: 28 | Loss : 0.013955509290099144\n",
      "Epoch: 29 | Loss : 0.013754991814494133\n",
      "Epoch: 30 | Loss : 0.013557259924709797\n",
      "Epoch: 31 | Loss : 0.013362398371100426\n",
      "Epoch: 32 | Loss : 0.013170382007956505\n",
      "Epoch: 33 | Loss : 0.012981119565665722\n",
      "Epoch: 34 | Loss : 0.012794509530067444\n",
      "Epoch: 35 | Loss : 0.012610653415322304\n",
      "Epoch: 36 | Loss : 0.012429431080818176\n",
      "Epoch: 37 | Loss : 0.012250827625393867\n",
      "Epoch: 38 | Loss : 0.012074684724211693\n",
      "Epoch: 39 | Loss : 0.01190122589468956\n",
      "Epoch: 40 | Loss : 0.011730212718248367\n",
      "Epoch: 41 | Loss : 0.011561567895114422\n",
      "Epoch: 42 | Loss : 0.011395379900932312\n",
      "Epoch: 43 | Loss : 0.01123163290321827\n",
      "Epoch: 44 | Loss : 0.011070264503359795\n",
      "Epoch: 45 | Loss : 0.010911114513874054\n",
      "Epoch: 46 | Loss : 0.010754291899502277\n",
      "Epoch: 47 | Loss : 0.01059974916279316\n",
      "Epoch: 48 | Loss : 0.010447405278682709\n",
      "Epoch: 49 | Loss : 0.01029730960726738\n",
      "Epoch: 50 | Loss : 0.0101493364199996\n",
      "Epoch: 51 | Loss : 0.01000344567000866\n",
      "Epoch: 52 | Loss : 0.009859644807875156\n",
      "Epoch: 53 | Loss : 0.009717957116663456\n",
      "Epoch: 54 | Loss : 0.00957831833511591\n",
      "Epoch: 55 | Loss : 0.009440663270652294\n",
      "Epoch: 56 | Loss : 0.009304962120950222\n",
      "Epoch: 57 | Loss : 0.009171229787170887\n",
      "Epoch: 58 | Loss : 0.009039432741701603\n",
      "Epoch: 59 | Loss : 0.008909577503800392\n",
      "Epoch: 60 | Loss : 0.008781501092016697\n",
      "Epoch: 61 | Loss : 0.008655281737446785\n",
      "Epoch: 62 | Loss : 0.008530905470252037\n",
      "Epoch: 63 | Loss : 0.008408303372561932\n",
      "Epoch: 64 | Loss : 0.008287462405860424\n",
      "Epoch: 65 | Loss : 0.008168363012373447\n",
      "Epoch: 66 | Loss : 0.008050934411585331\n",
      "Epoch: 67 | Loss : 0.00793526228517294\n",
      "Epoch: 68 | Loss : 0.007821219973266125\n",
      "Epoch: 69 | Loss : 0.007708808407187462\n",
      "Epoch: 70 | Loss : 0.007598006632179022\n",
      "Epoch: 71 | Loss : 0.007488842587918043\n",
      "Epoch: 72 | Loss : 0.007381192408502102\n",
      "Epoch: 73 | Loss : 0.007275149691849947\n",
      "Epoch: 74 | Loss : 0.0071705784648656845\n",
      "Epoch: 75 | Loss : 0.007067522034049034\n",
      "Epoch: 76 | Loss : 0.006965915206819773\n",
      "Epoch: 77 | Loss : 0.0068658581003546715\n",
      "Epoch: 78 | Loss : 0.006767175626009703\n",
      "Epoch: 79 | Loss : 0.006669914815574884\n",
      "Epoch: 80 | Loss : 0.006574050989001989\n",
      "Epoch: 81 | Loss : 0.006479564122855663\n",
      "Epoch: 82 | Loss : 0.006386435590684414\n",
      "Epoch: 83 | Loss : 0.006294691003859043\n",
      "Epoch: 84 | Loss : 0.006204247009009123\n",
      "Epoch: 85 | Loss : 0.006115039810538292\n",
      "Epoch: 86 | Loss : 0.006027164403349161\n",
      "Epoch: 87 | Loss : 0.005940538831055164\n",
      "Epoch: 88 | Loss : 0.00585515471175313\n",
      "Epoch: 89 | Loss : 0.005770979914814234\n",
      "Epoch: 90 | Loss : 0.005688099190592766\n",
      "Epoch: 91 | Loss : 0.005606335587799549\n",
      "Epoch: 92 | Loss : 0.0055257780477404594\n",
      "Epoch: 93 | Loss : 0.005446331109851599\n",
      "Epoch: 94 | Loss : 0.005368081387132406\n",
      "Epoch: 95 | Loss : 0.005290905479341745\n",
      "Epoch: 96 | Loss : 0.0052148932591080666\n",
      "Epoch: 97 | Loss : 0.005139943212270737\n",
      "Epoch: 98 | Loss : 0.005066102836281061\n",
      "Epoch: 99 | Loss : 0.004993269685655832\n",
      "Epoch: 100 | Loss : 0.004921517800539732\n",
      "Epoch: 101 | Loss : 0.004850788041949272\n",
      "Epoch: 102 | Loss : 0.004781072493642569\n",
      "Epoch: 103 | Loss : 0.004712348338216543\n",
      "Epoch: 104 | Loss : 0.004644649103283882\n",
      "Epoch: 105 | Loss : 0.004577876068651676\n",
      "Epoch: 106 | Loss : 0.004512082785367966\n",
      "Epoch: 107 | Loss : 0.00444723479449749\n",
      "Epoch: 108 | Loss : 0.004383310209959745\n",
      "Epoch: 109 | Loss : 0.004320345353335142\n",
      "Epoch: 110 | Loss : 0.004258215427398682\n",
      "Epoch: 111 | Loss : 0.004197033122181892\n",
      "Epoch: 112 | Loss : 0.004136726725846529\n",
      "Epoch: 113 | Loss : 0.004077280405908823\n",
      "Epoch: 114 | Loss : 0.004018702544271946\n",
      "Epoch: 115 | Loss : 0.003960925620049238\n",
      "Epoch: 116 | Loss : 0.003903985256329179\n",
      "Epoch: 117 | Loss : 0.003847911022603512\n",
      "Epoch: 118 | Loss : 0.0037925865035504103\n",
      "Epoch: 119 | Loss : 0.0037381139118224382\n",
      "Epoch: 120 | Loss : 0.0036843849811702967\n",
      "Epoch: 121 | Loss : 0.0036314267199486494\n",
      "Epoch: 122 | Loss : 0.0035792309790849686\n",
      "Epoch: 123 | Loss : 0.0035277896095067263\n",
      "Epoch: 124 | Loss : 0.0034770769998431206\n",
      "Epoch: 125 | Loss : 0.003427122952416539\n",
      "Epoch: 126 | Loss : 0.0033778550568968058\n",
      "Epoch: 127 | Loss : 0.0033292926382273436\n",
      "Epoch: 128 | Loss : 0.003281498095020652\n",
      "Epoch: 129 | Loss : 0.003234307048842311\n",
      "Epoch: 130 | Loss : 0.003187818918377161\n",
      "Epoch: 131 | Loss : 0.0031420420855283737\n",
      "Epoch: 132 | Loss : 0.003096848726272583\n",
      "Epoch: 133 | Loss : 0.0030523522291332483\n",
      "Epoch: 134 | Loss : 0.003008487168699503\n",
      "Epoch: 135 | Loss : 0.002965243300423026\n",
      "Epoch: 136 | Loss : 0.0029226457700133324\n",
      "Epoch: 137 | Loss : 0.002880631247535348\n",
      "Epoch: 138 | Loss : 0.0028392274398356676\n",
      "Epoch: 139 | Loss : 0.0027984187472611666\n",
      "Epoch: 140 | Loss : 0.0027582074981182814\n",
      "Epoch: 141 | Loss : 0.0027185967192053795\n",
      "Epoch: 142 | Loss : 0.0026794951409101486\n",
      "Epoch: 143 | Loss : 0.002640999387949705\n",
      "Epoch: 144 | Loss : 0.002603022614493966\n",
      "Epoch: 145 | Loss : 0.002565644681453705\n",
      "Epoch: 146 | Loss : 0.0025287624448537827\n",
      "Epoch: 147 | Loss : 0.002492431551218033\n",
      "Epoch: 148 | Loss : 0.002456611255183816\n",
      "Epoch: 149 | Loss : 0.0024212936405092478\n",
      "Epoch: 150 | Loss : 0.002386509906500578\n",
      "Epoch: 151 | Loss : 0.0023522062692791224\n",
      "Epoch: 152 | Loss : 0.0023183973971754313\n",
      "Epoch: 153 | Loss : 0.0022850611712783575\n",
      "Epoch: 154 | Loss : 0.0022522425279021263\n",
      "Epoch: 155 | Loss : 0.0022198581136763096\n",
      "Epoch: 156 | Loss : 0.00218795333057642\n",
      "Epoch: 157 | Loss : 0.00215653027407825\n",
      "Epoch: 158 | Loss : 0.002125511644408107\n",
      "Epoch: 159 | Loss : 0.002094973111525178\n",
      "Epoch: 160 | Loss : 0.0020648723002523184\n",
      "Epoch: 161 | Loss : 0.0020351833663880825\n",
      "Epoch: 162 | Loss : 0.002005946356803179\n",
      "Epoch: 163 | Loss : 0.001977133098989725\n",
      "Epoch: 164 | Loss : 0.0019487026147544384\n",
      "Epoch: 165 | Loss : 0.0019207072909921408\n",
      "Epoch: 166 | Loss : 0.0018931018421426415\n",
      "Epoch: 167 | Loss : 0.0018658996559679508\n",
      "Epoch: 168 | Loss : 0.0018390812911093235\n",
      "Epoch: 169 | Loss : 0.001812640344724059\n",
      "Epoch: 170 | Loss : 0.0017865775153040886\n",
      "Epoch: 171 | Loss : 0.0017609255155548453\n",
      "Epoch: 172 | Loss : 0.0017355945892632008\n",
      "Epoch: 173 | Loss : 0.0017106733284890652\n",
      "Epoch: 174 | Loss : 0.0016860731411725283\n",
      "Epoch: 175 | Loss : 0.0016618595691397786\n",
      "Epoch: 176 | Loss : 0.001637960085645318\n",
      "Epoch: 177 | Loss : 0.0016144323162734509\n",
      "Epoch: 178 | Loss : 0.001591227832250297\n",
      "Epoch: 179 | Loss : 0.0015683615347370505\n",
      "Epoch: 180 | Loss : 0.0015458117704838514\n",
      "Epoch: 181 | Loss : 0.001523604616522789\n",
      "Epoch: 182 | Loss : 0.0015017000259831548\n",
      "Epoch: 183 | Loss : 0.001480110571719706\n",
      "Epoch: 184 | Loss : 0.0014588374178856611\n",
      "Epoch: 185 | Loss : 0.001437887898646295\n",
      "Epoch: 186 | Loss : 0.0014172191731631756\n",
      "Epoch: 187 | Loss : 0.0013968652347102761\n",
      "Epoch: 188 | Loss : 0.0013767835916951299\n",
      "Epoch: 189 | Loss : 0.0013569950824603438\n",
      "Epoch: 190 | Loss : 0.0013374859699979424\n",
      "Epoch: 191 | Loss : 0.0013182659167796373\n",
      "Epoch: 192 | Loss : 0.0012993128038942814\n",
      "Epoch: 193 | Loss : 0.0012806592276319861\n",
      "Epoch: 194 | Loss : 0.0012622375506907701\n",
      "Epoch: 195 | Loss : 0.0012441073777154088\n",
      "Epoch: 196 | Loss : 0.0012262080563232303\n",
      "Epoch: 197 | Loss : 0.0012086081551387906\n",
      "Epoch: 198 | Loss : 0.0011912358459085226\n",
      "Epoch: 199 | Loss : 0.001174114877358079\n",
      "Epoch: 200 | Loss : 0.001157248392701149\n",
      "Epoch: 201 | Loss : 0.0011405922705307603\n",
      "Epoch: 202 | Loss : 0.0011242147302255034\n",
      "Epoch: 203 | Loss : 0.001108058262616396\n",
      "Epoch: 204 | Loss : 0.0010921340435743332\n",
      "Epoch: 205 | Loss : 0.001076428103260696\n",
      "Epoch: 206 | Loss : 0.0010609740857034922\n",
      "Epoch: 207 | Loss : 0.0010457258904352784\n",
      "Epoch: 208 | Loss : 0.0010306925978511572\n",
      "Epoch: 209 | Loss : 0.0010158626828342676\n",
      "Epoch: 210 | Loss : 0.0010012744460254908\n",
      "Epoch: 211 | Loss : 0.000986894010566175\n",
      "Epoch: 212 | Loss : 0.0009727083961479366\n",
      "Epoch: 213 | Loss : 0.0009587315726093948\n",
      "Epoch: 214 | Loss : 0.0009449631907045841\n",
      "Epoch: 215 | Loss : 0.0009313618065789342\n",
      "Epoch: 216 | Loss : 0.000917994708288461\n",
      "Epoch: 217 | Loss : 0.0009047854109667242\n",
      "Epoch: 218 | Loss : 0.0008917864179238677\n",
      "Epoch: 219 | Loss : 0.0008789642597548664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 220 | Loss : 0.0008663478656671941\n",
      "Epoch: 221 | Loss : 0.0008538821712136269\n",
      "Epoch: 222 | Loss : 0.0008416248601861298\n",
      "Epoch: 223 | Loss : 0.0008295315783470869\n",
      "Epoch: 224 | Loss : 0.000817610532976687\n",
      "Epoch: 225 | Loss : 0.0008058467647060752\n",
      "Epoch: 226 | Loss : 0.0007942731608636677\n",
      "Epoch: 227 | Loss : 0.0007828531670384109\n",
      "Epoch: 228 | Loss : 0.0007715966203249991\n",
      "Epoch: 229 | Loss : 0.0007605210412293673\n",
      "Epoch: 230 | Loss : 0.0007495876052416861\n",
      "Epoch: 231 | Loss : 0.0007388108060695231\n",
      "Epoch: 232 | Loss : 0.0007281872676685452\n",
      "Epoch: 233 | Loss : 0.000717729504685849\n",
      "Epoch: 234 | Loss : 0.0007074200548231602\n",
      "Epoch: 235 | Loss : 0.0006972389528527856\n",
      "Epoch: 236 | Loss : 0.0006872261292301118\n",
      "Epoch: 237 | Loss : 0.0006773507921025157\n",
      "Epoch: 238 | Loss : 0.0006676160264760256\n",
      "Epoch: 239 | Loss : 0.0006580173503607512\n",
      "Epoch: 240 | Loss : 0.0006485666381195188\n",
      "Epoch: 241 | Loss : 0.0006392416544258595\n",
      "Epoch: 242 | Loss : 0.0006300585810095072\n",
      "Epoch: 243 | Loss : 0.0006209926796145737\n",
      "Epoch: 244 | Loss : 0.000612071598879993\n",
      "Epoch: 245 | Loss : 0.0006032793899066746\n",
      "Epoch: 246 | Loss : 0.0005946031888015568\n",
      "Epoch: 247 | Loss : 0.000586061563808471\n",
      "Epoch: 248 | Loss : 0.0005776460748165846\n",
      "Epoch: 249 | Loss : 0.0005693414132110775\n",
      "Epoch: 250 | Loss : 0.0005611602100543678\n",
      "Epoch: 251 | Loss : 0.0005531011265702546\n",
      "Epoch: 252 | Loss : 0.0005451356410048902\n",
      "Epoch: 253 | Loss : 0.0005373047315515578\n",
      "Epoch: 254 | Loss : 0.0005295853479765356\n",
      "Epoch: 255 | Loss : 0.0005219762679189444\n",
      "Epoch: 256 | Loss : 0.0005144657916389406\n",
      "Epoch: 257 | Loss : 0.0005070712650194764\n",
      "Epoch: 258 | Loss : 0.0004997900687158108\n",
      "Epoch: 259 | Loss : 0.0004926080000586808\n",
      "Epoch: 260 | Loss : 0.00048552913358435035\n",
      "Epoch: 261 | Loss : 0.00047855492448434234\n",
      "Epoch: 262 | Loss : 0.000471679144538939\n",
      "Epoch: 263 | Loss : 0.00046489076339639723\n",
      "Epoch: 264 | Loss : 0.0004582150431815535\n",
      "Epoch: 265 | Loss : 0.00045163699542172253\n",
      "Epoch: 266 | Loss : 0.00044514090404845774\n",
      "Epoch: 267 | Loss : 0.00043873570393770933\n",
      "Epoch: 268 | Loss : 0.00043243373511359096\n",
      "Epoch: 269 | Loss : 0.0004262241709511727\n",
      "Epoch: 270 | Loss : 0.00042009170283563435\n",
      "Epoch: 271 | Loss : 0.0004140581004321575\n",
      "Epoch: 272 | Loss : 0.00040810101199895144\n",
      "Epoch: 273 | Loss : 0.0004022418870590627\n",
      "Epoch: 274 | Loss : 0.0003964610514231026\n",
      "Epoch: 275 | Loss : 0.0003907634236384183\n",
      "Epoch: 276 | Loss : 0.00038514810148626566\n",
      "Epoch: 277 | Loss : 0.0003796153177972883\n",
      "Epoch: 278 | Loss : 0.00037416085251607\n",
      "Epoch: 279 | Loss : 0.0003687826683744788\n",
      "Epoch: 280 | Loss : 0.0003634789027273655\n",
      "Epoch: 281 | Loss : 0.00035825427039526403\n",
      "Epoch: 282 | Loss : 0.00035310134990140796\n",
      "Epoch: 283 | Loss : 0.00034803885500878096\n",
      "Epoch: 284 | Loss : 0.0003430334327276796\n",
      "Epoch: 285 | Loss : 0.00033809951855801046\n",
      "Epoch: 286 | Loss : 0.00033324380638077855\n",
      "Epoch: 287 | Loss : 0.0003284474660176784\n",
      "Epoch: 288 | Loss : 0.00032372563146054745\n",
      "Epoch: 289 | Loss : 0.00031907434458844364\n",
      "Epoch: 290 | Loss : 0.0003144958755001426\n",
      "Epoch: 291 | Loss : 0.0003099740424659103\n",
      "Epoch: 292 | Loss : 0.00030552156385965645\n",
      "Epoch: 293 | Loss : 0.0003011265071108937\n",
      "Epoch: 294 | Loss : 0.000296799378702417\n",
      "Epoch: 295 | Loss : 0.0002925323788076639\n",
      "Epoch: 296 | Loss : 0.0002883278648369014\n",
      "Epoch: 297 | Loss : 0.00028418906731531024\n",
      "Epoch: 298 | Loss : 0.00028010064852423966\n",
      "Epoch: 299 | Loss : 0.0002760660136118531\n",
      "Epoch: 300 | Loss : 0.00027210282860323787\n",
      "Epoch: 301 | Loss : 0.0002681959012988955\n",
      "Epoch: 302 | Loss : 0.00026433708262629807\n",
      "Epoch: 303 | Loss : 0.00026054377667605877\n",
      "Epoch: 304 | Loss : 0.00025679744430817664\n",
      "Epoch: 305 | Loss : 0.0002531078353058547\n",
      "Epoch: 306 | Loss : 0.0002494642394594848\n",
      "Epoch: 307 | Loss : 0.00024588804808445275\n",
      "Epoch: 308 | Loss : 0.000242349473410286\n",
      "Epoch: 309 | Loss : 0.00023887430143076926\n",
      "Epoch: 310 | Loss : 0.00023542778217233717\n",
      "Epoch: 311 | Loss : 0.00023205234901979566\n",
      "Epoch: 312 | Loss : 0.00022871536202728748\n",
      "Epoch: 313 | Loss : 0.00022543570958077908\n",
      "Epoch: 314 | Loss : 0.00022219102538656443\n",
      "Epoch: 315 | Loss : 0.0002189973893109709\n",
      "Epoch: 316 | Loss : 0.00021585165814030915\n",
      "Epoch: 317 | Loss : 0.0002127422485500574\n",
      "Epoch: 318 | Loss : 0.00020969158504158258\n",
      "Epoch: 319 | Loss : 0.0002066763408947736\n",
      "Epoch: 320 | Loss : 0.0002037069934885949\n",
      "Epoch: 321 | Loss : 0.0002007772563956678\n",
      "Epoch: 322 | Loss : 0.00019790064834523946\n",
      "Epoch: 323 | Loss : 0.00019504399097058922\n",
      "Epoch: 324 | Loss : 0.00019224436255171895\n",
      "Epoch: 325 | Loss : 0.00018948338401969522\n",
      "Epoch: 326 | Loss : 0.00018676066247280687\n",
      "Epoch: 327 | Loss : 0.0001840742042986676\n",
      "Epoch: 328 | Loss : 0.00018143068882636726\n",
      "Epoch: 329 | Loss : 0.00017882419342640787\n",
      "Epoch: 330 | Loss : 0.00017625353939365596\n",
      "Epoch: 331 | Loss : 0.0001737244747346267\n",
      "Epoch: 332 | Loss : 0.00017122663848567754\n",
      "Epoch: 333 | Loss : 0.00016876803420018405\n",
      "Epoch: 334 | Loss : 0.00016633844643365592\n",
      "Epoch: 335 | Loss : 0.00016394807607866824\n",
      "Epoch: 336 | Loss : 0.00016159126244019717\n",
      "Epoch: 337 | Loss : 0.0001592691260157153\n",
      "Epoch: 338 | Loss : 0.00015697981871198863\n",
      "Epoch: 339 | Loss : 0.00015472299128305167\n",
      "Epoch: 340 | Loss : 0.00015249974967446178\n",
      "Epoch: 341 | Loss : 0.00015030827489681542\n",
      "Epoch: 342 | Loss : 0.00014815317990723997\n",
      "Epoch: 343 | Loss : 0.00014601855946239084\n",
      "Epoch: 344 | Loss : 0.00014392175944522023\n",
      "Epoch: 345 | Loss : 0.00014185467443894595\n",
      "Epoch: 346 | Loss : 0.00013981627125758678\n",
      "Epoch: 347 | Loss : 0.0001378056185785681\n",
      "Epoch: 348 | Loss : 0.00013582305109594017\n",
      "Epoch: 349 | Loss : 0.00013387101353146136\n",
      "Epoch: 350 | Loss : 0.0001319424482062459\n",
      "Epoch: 351 | Loss : 0.00013005240180063993\n",
      "Epoch: 352 | Loss : 0.00012818060349673033\n",
      "Epoch: 353 | Loss : 0.00012634329323191196\n",
      "Epoch: 354 | Loss : 0.00012452832015696913\n",
      "Epoch: 355 | Loss : 0.0001227355096489191\n",
      "Epoch: 356 | Loss : 0.00012097039143554866\n",
      "Epoch: 357 | Loss : 0.00011922822886845097\n",
      "Epoch: 358 | Loss : 0.00011751570855267346\n",
      "Epoch: 359 | Loss : 0.00011582622391870245\n",
      "Epoch: 360 | Loss : 0.00011415953485993668\n",
      "Epoch: 361 | Loss : 0.00011252650438109413\n",
      "Epoch: 362 | Loss : 0.00011090401676483452\n",
      "Epoch: 363 | Loss : 0.00010931336873909459\n",
      "Epoch: 364 | Loss : 0.00010774266411317512\n",
      "Epoch: 365 | Loss : 0.00010619171371217817\n",
      "Epoch: 366 | Loss : 0.0001046716351993382\n",
      "Epoch: 367 | Loss : 0.00010316429688828066\n",
      "Epoch: 368 | Loss : 0.00010168312292080373\n",
      "Epoch: 369 | Loss : 0.00010022023343481123\n",
      "Epoch: 370 | Loss : 9.878238779492676e-05\n",
      "Epoch: 371 | Loss : 9.736181527841836e-05\n",
      "Epoch: 372 | Loss : 9.596406016498804e-05\n",
      "Epoch: 373 | Loss : 9.458320710109547e-05\n",
      "Epoch: 374 | Loss : 9.322301775682718e-05\n",
      "Epoch: 375 | Loss : 9.188049443764612e-05\n",
      "Epoch: 376 | Loss : 9.056595445144922e-05\n",
      "Epoch: 377 | Loss : 8.926308510126546e-05\n",
      "Epoch: 378 | Loss : 8.7978936790023e-05\n",
      "Epoch: 379 | Loss : 8.671652904013172e-05\n",
      "Epoch: 380 | Loss : 8.54659519973211e-05\n",
      "Epoch: 381 | Loss : 8.423832332482561e-05\n",
      "Epoch: 382 | Loss : 8.302963396999985e-05\n",
      "Epoch: 383 | Loss : 8.183442696463317e-05\n",
      "Epoch: 384 | Loss : 8.065988367889076e-05\n",
      "Epoch: 385 | Loss : 7.950109284138307e-05\n",
      "Epoch: 386 | Loss : 7.835789438104257e-05\n",
      "Epoch: 387 | Loss : 7.723217277089134e-05\n",
      "Epoch: 388 | Loss : 7.612524495925754e-05\n",
      "Epoch: 389 | Loss : 7.502480002585799e-05\n",
      "Epoch: 390 | Loss : 7.394886779366061e-05\n",
      "Epoch: 391 | Loss : 7.288368215085939e-05\n",
      "Epoch: 392 | Loss : 7.184248534031212e-05\n",
      "Epoch: 393 | Loss : 7.080682553350925e-05\n",
      "Epoch: 394 | Loss : 6.979082536417991e-05\n",
      "Epoch: 395 | Loss : 6.878940621390939e-05\n",
      "Epoch: 396 | Loss : 6.779809336876497e-05\n",
      "Epoch: 397 | Loss : 6.682110688416287e-05\n",
      "Epoch: 398 | Loss : 6.586158997379243e-05\n",
      "Epoch: 399 | Loss : 6.491743988590315e-05\n",
      "Epoch: 400 | Loss : 6.398150435416028e-05\n",
      "Epoch: 401 | Loss : 6.306391151156276e-05\n",
      "Epoch: 402 | Loss : 6.215982284629717e-05\n",
      "Epoch: 403 | Loss : 6.126317020971328e-05\n",
      "Epoch: 404 | Loss : 6.038522769813426e-05\n",
      "Epoch: 405 | Loss : 5.951676212134771e-05\n",
      "Epoch: 406 | Loss : 5.866216088179499e-05\n",
      "Epoch: 407 | Loss : 5.781683648820035e-05\n",
      "Epoch: 408 | Loss : 5.698641689377837e-05\n",
      "Epoch: 409 | Loss : 5.6170294556068256e-05\n",
      "Epoch: 410 | Loss : 5.536221215152182e-05\n",
      "Epoch: 411 | Loss : 5.456256985780783e-05\n",
      "Epoch: 412 | Loss : 5.377940397011116e-05\n",
      "Epoch: 413 | Loss : 5.300866541801952e-05\n",
      "Epoch: 414 | Loss : 5.2244755352148786e-05\n",
      "Epoch: 415 | Loss : 5.14938983542379e-05\n",
      "Epoch: 416 | Loss : 5.075427543488331e-05\n",
      "Epoch: 417 | Loss : 5.002945908927359e-05\n",
      "Epoch: 418 | Loss : 4.930741124553606e-05\n",
      "Epoch: 419 | Loss : 4.859831096837297e-05\n",
      "Epoch: 420 | Loss : 4.7898374759824947e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 421 | Loss : 4.7212695790221915e-05\n",
      "Epoch: 422 | Loss : 4.653117503039539e-05\n",
      "Epoch: 423 | Loss : 4.586169961839914e-05\n",
      "Epoch: 424 | Loss : 4.520683432929218e-05\n",
      "Epoch: 425 | Loss : 4.455552334547974e-05\n",
      "Epoch: 426 | Loss : 4.391625407151878e-05\n",
      "Epoch: 427 | Loss : 4.328237992012873e-05\n",
      "Epoch: 428 | Loss : 4.266183896106668e-05\n",
      "Epoch: 429 | Loss : 4.20484175265301e-05\n",
      "Epoch: 430 | Loss : 4.144468402955681e-05\n",
      "Epoch: 431 | Loss : 4.084828469785862e-05\n",
      "Epoch: 432 | Loss : 4.026137321488932e-05\n",
      "Epoch: 433 | Loss : 3.9681272028246894e-05\n",
      "Epoch: 434 | Loss : 3.911156818503514e-05\n",
      "Epoch: 435 | Loss : 3.855175236822106e-05\n",
      "Epoch: 436 | Loss : 3.799812839133665e-05\n",
      "Epoch: 437 | Loss : 3.74509982066229e-05\n",
      "Epoch: 438 | Loss : 3.6913130315952e-05\n",
      "Epoch: 439 | Loss : 3.638090493041091e-05\n",
      "Epoch: 440 | Loss : 3.5859167837770656e-05\n",
      "Epoch: 441 | Loss : 3.5342920455150306e-05\n",
      "Epoch: 442 | Loss : 3.4836255508707836e-05\n",
      "Epoch: 443 | Loss : 3.4334945667069405e-05\n",
      "Epoch: 444 | Loss : 3.384032243047841e-05\n",
      "Epoch: 445 | Loss : 3.335431756568141e-05\n",
      "Epoch: 446 | Loss : 3.2877160265343264e-05\n",
      "Epoch: 447 | Loss : 3.240278601879254e-05\n",
      "Epoch: 448 | Loss : 3.1940071494318545e-05\n",
      "Epoch: 449 | Loss : 3.147742245346308e-05\n",
      "Epoch: 450 | Loss : 3.102753544226289e-05\n",
      "Epoch: 451 | Loss : 3.0581857572542503e-05\n",
      "Epoch: 452 | Loss : 3.0143230105750263e-05\n",
      "Epoch: 453 | Loss : 2.97058741125511e-05\n",
      "Epoch: 454 | Loss : 2.9281151000759564e-05\n",
      "Epoch: 455 | Loss : 2.885948561015539e-05\n",
      "Epoch: 456 | Loss : 2.8445527277654037e-05\n",
      "Epoch: 457 | Loss : 2.803671668516472e-05\n",
      "Epoch: 458 | Loss : 2.763392149063293e-05\n",
      "Epoch: 459 | Loss : 2.7237680114922114e-05\n",
      "Epoch: 460 | Loss : 2.684701576072257e-05\n",
      "Epoch: 461 | Loss : 2.6458275897311978e-05\n",
      "Epoch: 462 | Loss : 2.6077714210259728e-05\n",
      "Epoch: 463 | Loss : 2.5704328436404467e-05\n",
      "Epoch: 464 | Loss : 2.5335395548609085e-05\n",
      "Epoch: 465 | Loss : 2.497145214874763e-05\n",
      "Epoch: 466 | Loss : 2.461446638335474e-05\n",
      "Epoch: 467 | Loss : 2.4257760742329992e-05\n",
      "Epoch: 468 | Loss : 2.391076850472018e-05\n",
      "Epoch: 469 | Loss : 2.356514596613124e-05\n",
      "Epoch: 470 | Loss : 2.322708496649284e-05\n",
      "Epoch: 471 | Loss : 2.2892852939548902e-05\n",
      "Epoch: 472 | Loss : 2.256601510453038e-05\n",
      "Epoch: 473 | Loss : 2.2240983525989577e-05\n",
      "Epoch: 474 | Loss : 2.1922662199358456e-05\n",
      "Epoch: 475 | Loss : 2.160636540793348e-05\n",
      "Epoch: 476 | Loss : 2.129424683516845e-05\n",
      "Epoch: 477 | Loss : 2.0988394680898637e-05\n",
      "Epoch: 478 | Loss : 2.068951289402321e-05\n",
      "Epoch: 479 | Loss : 2.0389887140481733e-05\n",
      "Epoch: 480 | Loss : 2.0098965251236223e-05\n",
      "Epoch: 481 | Loss : 1.9807286662398838e-05\n",
      "Epoch: 482 | Loss : 1.9524417439242825e-05\n",
      "Epoch: 483 | Loss : 1.9244600480305962e-05\n",
      "Epoch: 484 | Loss : 1.8967566575156525e-05\n",
      "Epoch: 485 | Loss : 1.8694050595513545e-05\n",
      "Epoch: 486 | Loss : 1.8426511815050617e-05\n",
      "Epoch: 487 | Loss : 1.816065559978597e-05\n",
      "Epoch: 488 | Loss : 1.789845191524364e-05\n",
      "Epoch: 489 | Loss : 1.7642552847974002e-05\n",
      "Epoch: 490 | Loss : 1.7389464119332843e-05\n",
      "Epoch: 491 | Loss : 1.7140127965831198e-05\n",
      "Epoch: 492 | Loss : 1.68930728250416e-05\n",
      "Epoch: 493 | Loss : 1.6651370970066637e-05\n",
      "Epoch: 494 | Loss : 1.641117523831781e-05\n",
      "Epoch: 495 | Loss : 1.617389170860406e-05\n",
      "Epoch: 496 | Loss : 1.5942514437483624e-05\n",
      "Epoch: 497 | Loss : 1.571441680425778e-05\n",
      "Epoch: 498 | Loss : 1.5487737982766703e-05\n",
      "Epoch: 499 | Loss : 1.5266794434865005e-05\n",
      "Prediction (after training) 4 7.995508670806885\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch.autograd import Variable \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#data prep\n",
    "x_data = Variable(torch.Tensor([[1.0],[2.0],[3.0]]))\n",
    "y_data = Variable(torch.Tensor([[2.0],[4.0],[6.0]]))\n",
    "\n",
    "#model generation \n",
    "class Model(torch.nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    # torch.nn은 torch.neural network 를 의미한다. \n",
    "    # nn.Module은 여러개의 레이어와 output을 return하는 forward(input) 메소드를 \n",
    "    # 포함한다. \n",
    "    \n",
    "    # 모든 뉴럴 네트워크는 기본 클래스 nn.Module로부터 파생 \n",
    "    # 생성자(constructor)에서 사용하고자 하는 모든 레이어를 선언 \n",
    "    # forward(input)함수에서 입력으로부터 결과까지 모델이 어떻게 실행되는지 정의\n",
    "    \n",
    "    # torch.nn 은 mini-batch만을 지원한다. 전체 torch.nn 패키지는 mini-batch\n",
    "    # 형태인 입력만을 지우너하며, 단일 데이터는 입력으로 지원하지 않는다. \n",
    "    # 만약 단일 샘플(1개의 데이터)이 있다면 input.unsqueeze(0)을 사용하여 \n",
    "    # 가짜 임시 배치 차원을 추가\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear module \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        super(Model,self).__init__()\n",
    "        #torch.nn.Module 클래스를 초기화시킴 \n",
    "        \n",
    "        self.linear = torch.nn.Linear(1,1)  \n",
    "        #input label 1, output label 1 \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we acceapt a Variable of input data and \n",
    "        we must return of output data. We can use Modules defined in the \n",
    "        constructor as well as arbitraty operators on Variables \n",
    "        \n",
    "        \"\"\"\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred \n",
    "    \n",
    "#our model\n",
    "model = Model()\n",
    "\n",
    "# Construct our loss function and Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two \n",
    "# nn.Linear modules which are members of the model. \n",
    "\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "#criterion = torch.nn.MSELoss(size_average=False)  #prevous version이다. \n",
    "#arg에는 reduction과 size_average 를 가질 수 있지만 reduction이 최신 버전 \n",
    "#reduction은 none, mean, sum 값을 가지며 각각 출력값의 none, mean, sum을 의미 \n",
    "#한편, size_average를 False로 하면 출력값을(?) 평균으로 나누지 않는다.\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.01)\n",
    "#생성한 파리미터들을 옵티마이저에 전달, SGD를 이용(mini batch를 활용한 GD)하여 \n",
    "#optimizing을 수행한다. \n",
    "\n",
    "epoch_list = []\n",
    "loss_list = []\n",
    "\n",
    "#Training loop \n",
    "for epoch in range(500):\n",
    "    # 1) forward pass : Compute predicted y by passing x to the model \n",
    "    y_pred = model(x_data)\n",
    "    \n",
    "    # 2) Compute and print loss \n",
    "    loss = criterion(y_pred,y_data)\n",
    "    print(f'Epoch: {epoch} | Loss : {loss.item()}')\n",
    "    #loss는 (1,) 형태의 tensor이며, loss.item() 은 loss의 스칼라 값이다. \n",
    "    # 포맷 문자열 리터럴 ; f(ormat)' {variable} {variable}' 형식\n",
    "    epoch_list.append(epoch)\n",
    "    loss_list.append(loss.item())\n",
    "    \n",
    "    #Zero gradients, perform a backward pass, and update the weights. \n",
    "    optimizer.zero_grad() #역전파를 실행하기 전 gradient를 0으로 만든다 \n",
    "    loss.backward() #모델의 매개변수에 대한 손실의 변화도를 계산한다. \n",
    "    optimizer.step() #optimizer의 step 함수를 통해 매개변수를 update 한다. \n",
    "    \n",
    "    \n",
    "# After training \n",
    "hour_var = torch.Tensor([[4.0]])\n",
    "y_pred = model(hour_var)\n",
    "print(\"Prediction (after training)\", 4, model(hour_var).data[0][0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_list,loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.nn \n",
    "\n",
    "    # torch.nn은 torch.neural network 를 의미한다. \n",
    "    # nn.Module은 여러개의 레이어와 output을 return하는 forward(input) 메소드를 포함 \n",
    "    \n",
    "    # 모든 뉴럴 네트워크는 기본 클래스 nn.Module로부터 파생 \n",
    "    # 생성자(constructor)에서 사용하고자 하는 모든 레이어를 선언 \n",
    "    # forward(input)함수에서 입력으로부터 결과까지 모델이 어떻게 실행되는지 정의\n",
    "    \n",
    "    # torch.nn 은 mini-batch만을 지원. 전체 torch.nn 패키지는 mini-batch 형태 입력만 지원\n",
    "    # 단일 데이터는 입력으로 지원 XXX\n",
    "    # 만약 단일 샘플(1개의 데이터)이 있다면 input.unsqueeze(0)을 사용해, 가짜 임시 배치 차원 추가\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "develroom-pytorch",
   "language": "python",
   "name": "develroom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
