{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 tensor(0.9322)\n",
      "0 1 tensor(0.9841)\n",
      "0 2 tensor(0.8528)\n",
      "0 3 tensor(0.8780)\n",
      "0 4 tensor(0.8929)\n",
      "0 5 tensor(0.8623)\n",
      "0 6 tensor(0.7339)\n",
      "0 7 tensor(0.7703)\n",
      "0 8 tensor(0.7970)\n",
      "0 9 tensor(0.8302)\n",
      "0 10 tensor(0.7377)\n",
      "0 11 tensor(0.7167)\n",
      "0 12 tensor(0.7293)\n",
      "0 13 tensor(0.7030)\n",
      "0 14 tensor(0.7157)\n",
      "0 15 tensor(0.7044)\n",
      "0 16 tensor(0.7013)\n",
      "0 17 tensor(0.6916)\n",
      "0 18 tensor(0.6963)\n",
      "0 19 tensor(0.6901)\n",
      "0 20 tensor(0.6963)\n",
      "0 21 tensor(0.6884)\n",
      "0 22 tensor(0.6807)\n",
      "0 23 tensor(0.6835)\n",
      "1 0 tensor(0.6840)\n",
      "1 1 tensor(0.6672)\n",
      "1 2 tensor(0.6712)\n",
      "1 3 tensor(0.6919)\n",
      "1 4 tensor(0.6689)\n",
      "1 5 tensor(0.6793)\n",
      "1 6 tensor(0.6456)\n",
      "1 7 tensor(0.6463)\n",
      "1 8 tensor(0.6500)\n",
      "1 9 tensor(0.6573)\n",
      "1 10 tensor(0.6560)\n",
      "1 11 tensor(0.6549)\n",
      "1 12 tensor(0.6429)\n",
      "1 13 tensor(0.6526)\n",
      "1 14 tensor(0.6634)\n",
      "1 15 tensor(0.6389)\n",
      "1 16 tensor(0.7005)\n",
      "1 17 tensor(0.6268)\n",
      "1 18 tensor(0.6624)\n",
      "1 19 tensor(0.6361)\n",
      "1 20 tensor(0.6352)\n",
      "1 21 tensor(0.6617)\n",
      "1 22 tensor(0.6477)\n",
      "1 23 tensor(0.6695)\n"
     ]
    }
   ],
   "source": [
    "# References\n",
    "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/pytorch_basics/main.py\n",
    "# http://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class\n",
    "import torch \n",
    "import numpy as np \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "\n",
    "#데이터 로드와 전처리과정 \n",
    "#DiabetesDataset 클래스 생성 \n",
    "class DiabetesDataset(Dataset):\n",
    "    \"\"\" Diabetes dataset.\"\"\"\n",
    "    \n",
    "    # Initialize your data, download, etc.\n",
    "    def __init__(self):\n",
    "        xy = np.loadtxt('diabetes.csv.gz',\n",
    "                        delimiter=',', dtype=np.float32) # 해당 파일로부터 데이터를 불러온다.\n",
    "        self.len = xy.shape[0] #길이 정보 저장\n",
    "        self.x_data = torch.from_numpy(xy[:, 0:-1]) #마지막 열 빼고 x_data에 저장\n",
    "        self.y_data = torch.from_numpy(xy[:, [-1]]) #마지막 열은 class label로 y_data에 저장 \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index] # 인덱스에 하나의 아이템 반환 \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len # 데이터 길이 반환 \n",
    "\n",
    "\n",
    "dataset = DiabetesDataset() #인스탠스 생성 \n",
    "train_loader = DataLoader(dataset=dataset, #dataset instance\n",
    "                          batch_size=32, # 배치사이즈 32\n",
    "                          shuffle=True,  # shuffling 수행\n",
    "                          num_workers=0) #원래, num_workers = 2  >> subprocess 2개 \n",
    "                                         # 하지만 2로 하니 runtimeerror가 발생했고 이를 0으로 고치니 작동이 되었음. \n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(8, 6) # 입력층 8개 레이블 , 은닉층 출력 6개 레이블\n",
    "        self.l2 = torch.nn.Linear(6, 4) #은닉 입력 6, 은닉 입력 4 \n",
    "        self.l3 = torch.nn.Linear(4, 1) # 은닉 입력 4, 최종 출력층 레이블 1개\n",
    "\n",
    "        self.sigmoid = torch.nn.Sigmoid() # 활성함수 시그모이드 사용 \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        out1 = self.sigmoid(self.l1(x)) #해당 층에서 전방계산이후 활성함수 적용 \n",
    "        out2 = self.sigmoid(self.l2(out1)) # 활성함수 적용 \n",
    "        y_pred = self.sigmoid(self.l3(out2)) # 활성함수 적용 \n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# our model\n",
    "model = Model()\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.BCELoss(size_average=True) #목적함수 바이너리 크로스 엔트로피 사용 \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1) # 최적화 스토캐스틱 그래디언드 디센트 사용 \n",
    "\n",
    "# Training loop\n",
    "for epoch in range(2): # 총 2 epoch\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        \n",
    "        # get the inputs\n",
    "        inputs, labels = data # 인풋과 레이블 받기 \n",
    "        \n",
    "        #wrap them in Variable \n",
    "        inputs, labels = Variable(inputs), Variable(labels) # Variable() 감싸기\n",
    "\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        y_pred = model(inputs) # 예측된 y \n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = criterion(y_pred, labels) # 목적함수 값 \n",
    "        print(epoch, i, loss.data)\n",
    "\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad() #초기화 \n",
    "        loss.backward() #백프로페게이션 \n",
    "        optimizer.step() #변수 업데이트 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) DataLoader 의 batch_size는 우리가 모델을 학습시킬 때 설정한 배치의 사이즈를 말한다. shuffle은 original data로부터 shuffling을 할 것인지를 묻는 것이고, num_worker 는  데이터를 로드하는데 얼마나 많은 subprocess들을 둘 것인지를 의미한다. 디폴트 값은 0으로 main process에서 데이터를 로드할 것임을 의미한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________\n",
    "++ 영상내용 \n",
    "\n",
    "데이터 로드 \n",
    "현재의 데이터셋은 700개 가량으로 모두 피팅하는데 문제가 없지만 데이터셋이 \n",
    "많아지면 모두 피팅하기 어려움 \n",
    "\n",
    "그래서 배치를 씀  \n",
    "\n",
    "epoch는 모든 트레이닝 데이터가 학습에 한번 forward backword 되는 것을 말하고 \n",
    "batch size는 트레이닝 데이터를 몇개씩 나눌것인지를 말한다. \n",
    "iteration은 할당된 배치 사이즈의 하나의 배치가 학습에 이용되는 것을 말한다. \n",
    "예를 들어, 1000개의 트레닝 데이터가 있고 배치사이즈가 500이면 2개의 배치가 만들어지고 에포크 한번을 cover하기 위해 2번의 iteration이 필요하게 된다. \n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "develroom-pytorch",
   "language": "python",
   "name": "develroom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
