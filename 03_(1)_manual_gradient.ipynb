{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.1"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## manual gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Prediction (before training) 4 4.0\n\tgrad :  1.0 2.0 -2.0\n\tgrad :  2.0 4.0 -7.84\n\tgrad :  3.0 6.0 -16.23\nprogress: 0 w= 1.26 loss= 4.92\n\tgrad :  1.0 2.0 -1.48\n\tgrad :  2.0 4.0 -5.8\n\tgrad :  3.0 6.0 -12.0\nprogress: 1 w= 1.45 loss= 2.69\n\tgrad :  1.0 2.0 -1.09\n\tgrad :  2.0 4.0 -4.29\n\tgrad :  3.0 6.0 -8.87\nprogress: 2 w= 1.6 loss= 1.47\n\tgrad :  1.0 2.0 -0.81\n\tgrad :  2.0 4.0 -3.17\n\tgrad :  3.0 6.0 -6.56\nprogress: 3 w= 1.7 loss= 0.8\n\tgrad :  1.0 2.0 -0.6\n\tgrad :  2.0 4.0 -2.34\n\tgrad :  3.0 6.0 -4.85\nprogress: 4 w= 1.78 loss= 0.44\n\tgrad :  1.0 2.0 -0.44\n\tgrad :  2.0 4.0 -1.73\n\tgrad :  3.0 6.0 -3.58\nprogress: 5 w= 1.84 loss= 0.24\n\tgrad :  1.0 2.0 -0.33\n\tgrad :  2.0 4.0 -1.28\n\tgrad :  3.0 6.0 -2.65\nprogress: 6 w= 1.88 loss= 0.13\n\tgrad :  1.0 2.0 -0.24\n\tgrad :  2.0 4.0 -0.95\n\tgrad :  3.0 6.0 -1.96\nprogress: 7 w= 1.91 loss= 0.07\n\tgrad :  1.0 2.0 -0.18\n\tgrad :  2.0 4.0 -0.7\n\tgrad :  3.0 6.0 -1.45\nprogress: 8 w= 1.93 loss= 0.04\n\tgrad :  1.0 2.0 -0.13\n\tgrad :  2.0 4.0 -0.52\n\tgrad :  3.0 6.0 -1.07\nprogress: 9 w= 1.95 loss= 0.02\nPredicted score (after training) 4 hours of studying:  7.804863933862125\n"
    }
   ],
   "source": [
    "#Traning data \n",
    "x_data = [1.0,2.0,3.0]\n",
    "y_data = [2.0,4.0,6.0]\n",
    "\n",
    "w = 1.0 # a random guess : random value \n",
    "\n",
    "# our model forward pass \n",
    "def forward(x): \n",
    "    return x*w\n",
    "\n",
    "# Loss function \n",
    "def loss(x,y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred-y)*(y_pred-y) #Square Error loss\n",
    "\n",
    "# compute gradient \n",
    "def gradient(x,y):\n",
    "    return 2*x*(x*w-y) # loss 값을 w에 대해 미분 ; 2w(wx-y)\n",
    "\n",
    "# Before training \n",
    "print (\"Prediction (before training)\", 4, forward(4)) \n",
    "\n",
    "# Training loop \n",
    "for epoch in range(10):\n",
    "    for x_val,y_val in zip(x_data,y_data):\n",
    "        # compute derivate w.r.t to the learned weights\n",
    "        # update \n",
    "        # compute the loss and print progress \n",
    "        grad = gradient(x_val,y_val)\n",
    "        w = w - 0.01 * grad # 학습률이 0.01 , grad가 반대 방향으로 update\n",
    "        print(\"\\tgrad : \",x_val, y_val, round(grad,2)) # x값, y값, grad값 \n",
    "        l = loss(x_val,y_val)\n",
    "    print(\"progress:\",epoch,\"w=\",round(w,2),\"loss=\",round(l,2)) \n",
    " \n",
    " #After training \n",
    "print(\"Predicted score (after training)\",\"4 hours of studying: \",forward(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "요약 : <br>\n",
    "forward, loss, gradient 를 정의한 뒤, 총 10번의 epoch를 수행한다. <br>\n",
    "한 번의 epoch는 zip 내의 모든 데이터를 통해 학습하는 것을 한 번의 epoch로 한다. <br>\n",
    "for epoch 내에 for zip 을 돌리고 그 안에서 grad값을 받고, w값을 update, 그리고 <br>\n",
    "그 값으로 새로운 forward 를 수행한 loss 값을 구해 나간다. epoch 횟수가 다 차면 학습을 끝낸다. "
   ]
  }
 ]
}